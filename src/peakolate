#!/usr/bin/env python

#
#                                   _         _       _             
#                  _ __   ___  __ _| | _____ | | __ _| |_ ___  _ __ 
#                 | '_ \ / _ \/ _` | |/ / _ \| |/ _` | __/ _ \| '__|
#                 | |_) |  __/ (_| |   < (_) | | (_| | || (_) | |   
#                 | .__/ \___|\__,_|_|\_\___/|_|\__,_|\__\___/|_|   
#                 |_|     
#
#                                      ~~~
#
#                          Finding enriched regions in 
#                          high-throughput sequencing data.
#
#                                      ~~~
#
#                             Copyright (C) 2010 by
#                    Daniel Jones <dcjones@cs.washington.edu>
# 
#
#                       (See included README for licensing.)
#


import time
import peakolator
import optparse
import operator
import re
import random
import bisect
import Queue
import multiprocessing
import argparse
from Queue           import Empty
from collections     import defaultdict, namedtuple
from sys             import argv, stderr, stdout, stdin
from os.path         import isfile
from multiprocessing import Process, Queue, JoinableQueue
from functools       import partial
from itertools       import izip, chain


msg = peakolator.logger()


#
#                           Two Useful Classes
#                           ------------------



# For debugging: pretend to be a joinable queue.
class dummy_queue:

    def empty(self):
        return len(self.xs) == 0

    def join(self):
        pass

    def close(self):
        pass

    def __init__(self):
        self.xs = list()

    def put(self,x):
        self.xs.append(x)

    def get(self, block = True, timeout = 0 ):
        return self.xs.pop(0)

    def task_done(self):
        pass


# A class to choose items from a list with non-uniform probability.
class weighted_choice:
    def __init__( self, xs, ws ):
        assert len(xs) == len(ws)

        self.ws = [0] * len(xs)
        total = float(sum(ws))
        accum = 0
        for (i,w) in enumerate(ws):
            accum += w/total if total != 0.0 else 0.0
            self.ws[i] = accum

        self.xs = xs

    def choose( self ):
        i = bisect.bisect_left( self.ws, random.uniform(0,1) )
        if i >= len(self.xs):
            return self.xs[-1]
        else:
            return self.xs[i]





#
#                           Annotation Juggling
#                           -------------------


gtf_row = namedtuple( 'gtf_row', 'seqname source feature start end ' \
                                 'score strand frame attributes' )

def gtf_row_from_gtf_line( line ):
    line = line.split('\t')
    if len(line) < 9:
        raise TypeError

    seqname = line[0]
    source  = line[1]
    feature = line[2]

    # gff/gtf is 1-based, end-inclusive
    start   = int(line[3])-1
    end     = int(line[4])-1

    score   = line[5]
    if line[6] == '+':
        strand = 0
    elif line[6] == '-':
        strand = 1
    else:
        strand = -1
    frame      = line[7]
    attributes = {}

    for mat in re.finditer( r'\s*(\w+)\s+(([\.\w]+)|"([\.\w]+)")\s*;', line[8] ):
        attributes[mat.group(1)] = mat.group(2).strip('"')

    return gtf_row( seqname, source, feature, \
                    start, end, score, strand, frame, \
                    attributes )



def gtf_row_from_bed_line( line ):
    line = line.split('\t')
    if len(line) < 3:
        raise TypeError

    seqname = line[0]

    # bed is 0-based, end-exclusive
    start   = int(line[1])
    end     = int(line[2])-1

    name   = line[3] if len(line) > 3 else ''
    score  = line[4] if len(line) > 4 else ''
    strand = line[5] if len(line) > 5 else ''
    if strand == '+':
        strand = 0
    elif strand == '-':
        strand = 1
    else:
        strand = -1

    return gtf_row( seqname, '', name, start, end, score, strand, '', {} )




def parse_gtf_bed( fn ):
    ''' try to determine the filetype and then parse it '''

    if re.search( 'bed$', fn ):
        return parse_bed( fn )
    if re.search( 'gtf$', fn ):
        return parse_gtf( fn )

    return parse_bed( fn )



def parse_gtf( gtf_fn, row_filter=lambda row: row.feature == 'exon' ):
    ''' extract annotations from a gtf file '''

    msg.write( 'parsing %s ... ' % gtf_fn )

    rows = []
    i = 0
    for line in open(gtf_fn):
        i+=1

        try:
            row = gtf_row_from_gtf_line( line )
        except TypeError:
            msg.write( 'Only %d fields found on line %d of %s. Skipping.\n' % \
                       (len(line.split('\t')), i, gtf_fn), peakolator.LOG_WARN )
            continue

        if row_filter(row):
            rows.append( row )

    msg.write( 'done (%d annotations).\n' % len(rows) )

    return rows




def parse_bed( bed_fn ):
    ''' extract annotations from a bed file '''

    msg.write( 'parsing %s ... ' % bed_fn )

    rows = []
    i = 0
    for line in open(bed_fn):
        i+=1

        try:
            row = gtf_row_from_bed_line( line )
        except TypeError:
            msg.write( 'Only %d fields found on line %d of %s. Skipping.\n' % \
                       (len(line.split('\t')), i, gtf_fn), peakolator.LOG_WARN )
            continue

        rows.append(row)

    msg.write( 'done (%d annotations).\n' % len(rows) )

    return rows


def print_bed( f, rows ):
    for row in rows:
        if row.strand == 0:
            strand = '+'
        elif row.strand == 1:
            strand = '-'
        else:
            strand = '.'

        f.write( '{seqname}\t{start}\t{end}\t{name}\t{score}\t{strand}\n'.format(
                    seqname = row.seqname,
                    start   = row.start,
                    end     = row.end+1,
                    name    = row.feature,
                    score   = row.score,
                    strand  = strand ) )




def gtf_intersect_rows( rows1, rows2, stranded = True ):
    ''' Perform a basewise intersection of two list of rows, producing a third
    that contains only intervals present in both sets. '''

    if stranded:
        key = lambda row: (row.strand, row.seqname)
    else:
        key = lambda row: row.seqname

    rows3 = []

    rows1.sort( key = lambda row: row.start )
    rows2.sort( key = lambda row: row.start )
    rows1.sort( key = key )
    rows2.sort( key = key )

    (i,j) = (0,0)
    while i < len(rows1) and j < len(rows2):
        if key(rows1[i]) < key(rows2[j]):
            i += 1
        elif key(rows1[i]) > key(rows2[j]):
            j += 1
        elif rows1[i].end < rows2[j].start:
            i += 1
        elif rows1[i].start > rows2[j].end:
            j += 1
        else: # intersection case
            row = gtf_row( seqname    = rows1[i].seqname,
                           source     = rows1[i].source,
                           feature    = rows1[i].feature,
                           start      = max( rows1[i].start, rows2[j].start ),
                           end        = min( rows1[i].end,   rows2[j].end ),
                           score      = rows1[i].score,
                           strand     = rows1[i].strand,
                           frame      = rows1[i].frame,
                           attributes = rows1[i].attributes )

            if rows1[i].end < rows2[j].end:
                i += 1
            elif rows1[i].end > rows2[j].end:
                j += 1
            else:
                i += 1
                j += 1

            rows3.append(row)

    return rows3




def get_constitutive_exons( rows, stranded = True ):
    ''' A dictionary mappin gene_id's to gtf_rows representing exonic intervals
    that are present in every isoform of the gene.'''

    msg.write( 'getting genes and transcripts ... ' )

    genes       = defaultdict(set)
    transcripts = defaultdict(list)

    for row in rows:
        if 'transcript_id' not in row.attributes or \
                 'gene_id' not in row.attributes:
            continue

        transcript_id = row.attributes['transcript_id']
        gene_id       = row.attributes['gene_id']

        genes[gene_id].add( transcript_id )
        transcripts[transcript_id].append( row )

    msg.write( 'done. (%d genes, %d transcripts)\n' % (len(genes), len(transcripts) ) )

    msg.write( 'getting constitutive exons ... ' )

    exons = {}

    for gene_id in genes:
        intersect = partial( gtf_intersect_rows, stranded = stranded )
        exons[gene_id] = reduce( intersect, [ transcripts[id] for id in genes[gene_id] ] )

        if not stranded:
            for (i,exon) in enumerate(exons[gene_id]):
                exons[gene_id][i] = exon._replace( strand = -1 )

    msg.write( 'done. (%d constitutive exons)\n' % \
                    sum( map( len, exons.itervalues() ) ) )

    return exons


def get_constitutive_introns( rows, stranded = True ):
    ''' Return a dictionary mapping gene_id's to a gtf_rows representing
    intronic intervals present in at least one isoform of a gene and not
    overlapping any exon from any gene. '''

    msg.write( 'getting constitutive introns ... ' )

    if stranded:
        key = lambda row: (row.strand, row.seqname, row.start, row.end)
    else:
        key = lambda row: (row.seqname, row.start, row.end)

    rows.sort( key = key )

    # count rows in each gene
    num_exons = defaultdict( lambda: 0 )
    for row in rows:
        if 'gene_id' in row.attributes:
            gene_id = row.attributes['gene_id']
            num_exons[gene_id] += 1


    # find constitutive introns
    introns   = defaultdict(list)
    curr_gene = []

    for (row1,row2) in izip( rows[:-1], rows[1:] ):
        if 'gene_id' in row1.attributes:
            gene_id = row1.attributes['gene_id']
            num_exons[gene_id] -= 1
            if not curr_gene or curr_gene[-1] != gene_id:
                curr_gene.append(gene_id)

            while curr_gene and num_exons[curr_gene[-1]] == 0:
                    curr_gene.pop()

        if row1.seqname != row2.seqname or \
           (stranded and row1.strand != row2.strand):
           continue


        # either ambiguous intron or outside of any gene
        if len(curr_gene) > 1 or len(curr_gene) == 0:
            continue

        if row1.end < row2.start:
            row = gtf_row( seqname    = row1.seqname,
                           source     = row1.source,
                           feature    = row1.feature,
                           start      = row1.end+1,
                           end        = row2.start-1,
                           score      = row1.score,
                           strand     = row1.strand if stranded else -1,
                           frame      = row1.frame,
                           attributes = row1.attributes )

            introns[curr_gene[-1]].append(row)

    msg.write( 'done. (%d constitutive introns)\n' % \
            sum( map( len, introns.itervalues() ) ) )

    return introns



def min_start_max_end( rows ):
    ''' return the minimum start and maximum end in a list of rows '''

    min_start = rows[0].start
    max_end   = rows[0].end
    for row in rows:
        if row.start < min_start:
            min_start = row.start
        if row.end > max_end:
            max_end   = row.end

    return (min_start,max_end)



def get_transcript_extents( rows ):
    ''' return a dictionary of sorted intervals for each chromosome, containing
    the extents of each transcript '''

    msg.write( 'getting transcript extents ... ' )

    # organize rows by transcript
    dummy_trans  = 1
    transcripts = defaultdict(list)
    for row in rows:
        if 'transcript_id' in row.attributes:
            transcripts[row.attributes['transcript_id']].append( row )
        else:
            transcripts['dummy_%d' % dummy_trans].append( row )
            dummy_trans += 1


    # get the extents of each transcript
    n = 1
    intervals = defaultdict(list)
    for transcript in transcripts.itervalues():
        transcript_intervals = defaultdict(list)
        k = 0
        for row in transcript:
            transcript_intervals[(row.seqname,row.strand)].append( row )

        for (seqname,strand) in transcript_intervals:
            (min_start,max_end) = min_start_max_end( transcript_intervals[(seqname,strand)] )
            k+=1
            intervals[seqname].append( (min_start,max_end,strand) )
        n += k

    # sort
    for seqname in intervals:
        intervals[seqname].sort( key = lambda x: x[0] )

    msg.write( 'done (%d transcripts)\n' % sum(map(len,intervals.itervalues())) )

    return intervals



def get_interval_gaps( intervals, chrom_sizes, stranded ):
    ''' get all the intervals, not covered by the provided intervals '''

    msg.write( 'getting intergenic regions ... ' )

    gaps = defaultdict(list)


    for seqname in chrom_sizes:
        if seqname not in intervals:
            if stranded:
                gaps[seqname].append( (0,chrom_sizes[seqname]-1,0) )
                gaps[seqname].append( (0,chrom_sizes[seqname]-1,1) )
            else:
                gaps[seqname].append( (0,chrom_sizes[seqname]-1,-1) )
            continue

        for s in ((0,1) if stranded else (-1,)):
            prev_end = -1
            for (start,end,strand) in sorted(intervals[seqname], key = lambda x: x[0]):
                if s != -1 and strand != -1 and s != strand: continue
                if prev_end+1 < start:
                    gaps[seqname].append( (prev_end+1,start-1,s) )
                prev_end = max( prev_end, end )

            if seqname in chrom_sizes and prev_end+1 < chrom_sizes[seqname]:
                gaps[seqname].append( (prev_end+1,chrom_sizes[seqname]-1,s) )

    msg.write( 'done (%d regions)\n' % sum(map(len,gaps.itervalues())) )

    return gaps


def random_training_examples( orig_regions, d, n = 10000, stranded=True ):
    '''
    Given a big set of regions, choose some random exmaple regions on which
    to train the null model.
    '''


    # Avoid sampling regions very near to known genes. These regions are always
    # enriched with reads. So constrict the defined intergenic regions.
    m = 2000

    regions = defaultdict(list)

    for seqname in orig_regions:
        for (start,end,strand) in orig_regions[seqname]:

            if not stranded: strand = -1

            if end-start+1 > 2*m+d:
                regions[seqname].append( (start+m,end-m,strand) )



    # the idea here is to do true uniform sampling by choosing a chromosome in
    # proportion to it's length, then choose a region proportionally to it's
    # length

    seqs = regions.keys()
    seq_weights = []
    for seqname in seqs:
        seq_weights.append( sum( end-start+1 for (start,end,strand) in regions[seqname]) )

    seq_chooser = weighted_choice( seqs, seq_weights )

    region_choosers = {}
    for seqname in seqs:
        ws = [ (end-start+1 if end-start+1 >= d else 0) \
             for (start,end,strand) in regions[seqname] ]
        region_choosers[seqname] = weighted_choice( regions[seqname], ws )

    train = []
    while len(train) < n:
        seqname = seq_chooser.choose()
        (start,end,strand) = region_choosers[seqname].choose()

        if end-start+1 < d: continue # shouldn't happen, but just in case

        i = int(random.uniform( 0, end-(d-1) ))

        train.append( peakolator.interval( seqname, i, i+(d-1), strand ) )


    return train







#
#                         Scannings 
#                         ---------
#


def peakolate( region_queue, prediction_queue, data, params ):
    '''
    One peakolator process: take regions off the queue, build the model and
    scan, pushing predictions onto another queue.
    '''

    ctx = peakolator.context()

    for R in iter(region_queue.get,None):

        (seqname,start,end,strand) = R

        msg.write( 'got one of length %d.\n' % (end-start+1), peakolator.LOG_BLAB )

        ctx.set( data, seqname, start, end, strand )

        M = peakolator.scanner( params, ctx )

        prediction_queue.put(
                [ (I.seqname, I.start, I.end, I.strand, I.score, 'peakolator')
                  for I in M.run() ] )

    msg.write( 'done!' )



def peakolate_intron( gene_queue, prediction_queue, data, params ):
    '''
    A specialized peakolator process to scan introns, recalibrating the
    model to the gene's constitutive exons.
    '''

    r = params.r
    p = params.p
    mu = r * ( 1 - p ) / p

    ctx = peakolator.context()

    for R in iter(gene_queue.get,None):
        (exons,introns) = R

        if len(introns) == 0 or len(exons) == 0:
            gene_queue.task_done()
            continue

        gene_id = introns[0].attributes['gene_id']

        msg.write( 'got gene "%s" with %d introns.\n' % (gene_id, len(introns) ) )

        exonic_rate  = 0
        exonic_count = 0
        exonic_len   = 0
        for exon in exons:
            ctx.set( data, exon.seqname, exon.start, exon.end, exon.strand )
            exonic_rate  += ctx.rate()
            exonic_count += ctx.count()
            exonic_len   += exon.end - exon.start + 1

        # skip over genes with exons that are too short to accurately calibrate
        # the model
        if exonic_len < 150:
            gene_queue.task_done()
            continue


        # scale model parameter to adjust expectation to that of constitutive
        # exons, and rebuild a (small) lookup table
        gamma = 2.0
        params.r = r * max( 1.0, gamma * (exonic_count / exonic_rate) / mu )
        params.rebuild_lookup( 500, 10 )

        for intron in introns:
            ctx.set( data, intron.seqname, intron.start, intron.end, intron.strand )
            M = peakolator.scanner( params, ctx )
            for I in M.run():
                prediction_queue.put(
                        (I.seqname, I.start, I.end,
                         I.strand, I.score, 'peakolator_intronic' ) )


        gene_queue.task_done()

    gene_queue.task_done() # complete the None task


def print_predictions( prediction_queue, f_out ):
    '''
    A process that prints any predictions it finds on the queue. This way we
    get results immediately.
    '''

    for prediction in iter(prediction_queue.get,None):
        (seqname,start,end,strand,pval,name) = prediction

        f_out.write( '{chrom}\t{start}\t{end}\t{name}\t{score}'.format(
            chrom = seqname,
            start = start,
            end   = end+1,   # adjust for end-exclusiveness of bed
            name  = name,
            score = pval ) )
        if strand >= 0:
            f_out.write( '\t%s' % ('+' if strand == 0 else '-') )
        f_out.write( '\n' )
        f_out.flush()
        prediction_queue.task_done()

    prediction_queue.task_done() # complete the None task
    msg.write( '(print_preditions) process terminated\n' )


def scan( args, data, chrom_sizes, regions ):
    '''
    Scan the intervels in regions, where regions is a dictionary mapping
    sequence names to tuples of the form (start, end, strand).
    '''

    # parameters (including a few magic numbers)
    params = peakolator.parameters()
    params.alpha = 0.01    # significance level
    params.d_min = 50      # minimum duration
    params.d_max = 100000  # maximum duration

    d_train = 100 # duration of training examples
    n_train = 50000 # number of training examples


    # train background model
    training_examples = random_training_examples( regions, d_train, n_train,
                                                  stranded = args.stranded )

    params.r, params.p = data.fit_null_distr( training_examples )



    # emperical p-values
    params.build_padj()


    # a single process version with no reporting, to simplify debugging
    if args.debug:
        msg.write( 'Warning: scanning in debug mode\n' )
        prediction_queue = dummy_queue()
        region_queue     = dummy_queue()

        for seqname in regions:
            if seqname not in chrom_sizes: continue

            # push all regions
            for (start,end,strand) in regions[seqname]:
                if end - start + 1 < params.d_min: continue

                region_queue.put( (seqname, start, end, strand) )

        region_queue.put( None )
        peakolate( region_queue, prediction_queue, data.copy(), params.copy() )

        predictions = []
        for prediction in iter(prediction_queue.get,None):
            predictions.append( prediction )

        return predictions




    prediction_queue = Queue()
    region_queue     = Queue()
    n = 0

    for seqname in regions:
        if seqname not in chrom_sizes: continue

        # push all regions
        for (start, end, strand) in regions[seqname]:
            if end - start + 1 < params.d_min: continue
            region_queue.put( (seqname, start, end, strand) )
            n += 1


    for _ in range(args.num_procs):
        t = Process( target = peakolate,
                     args = (region_queue, prediction_queue,
                             data.copy(), params.copy() ) )
        t.start()


    predictions = []
    for i in xrange(n):
        predictions.extend( prediction_queue.get() )


    # end scanning processes
    for _ in range(args.num_procs):
        region_queue.put( None )

    return predictions



def main():
    desc = 'Given aligned reads, scan the genome for regions ' \
           'sequenced at a rate beyond low level noise.'

    ap = argparse.ArgumentParser( description = desc )

    ap.add_argument( 'reads_fn', metavar = 'reads.bam',
                     help = 'aligned reads in bam format, indexed '
                            'with \'samtools index\'' )

    ap.add_argument( '-i', '--ignore', action = 'append', metavar = 'in.bed/gtf',
                     default = [],
                     help = 'regions in BED or GTF format to ignore in the scan')

    ap.add_argument( '-v', '--verbose', action = 'store_true', default = False,
                     help = 'print a lot of output' )

    ap.add_argument( '-p', '--processes', action = 'store',
                     metavar = 'n', dest='num_procs', type=int, default=1,
                     help = 'number of processes to use concurrently' )

    ap.add_argument( '-S', '--unstranded', action = 'store_false',
                     dest = 'stranded', default = True,
                     help = 'reads are not strand specific' )

    ap.add_argument( '-b', '--bias-correction', metavar = 'model.yml',
                     default = None, dest = 'bias_fn',
                     help = 'perform bias correction using the given model, '
                            'trained with peakolate-seqbias' )

    ap.add_argument( '-r', '--reference', metavar = 'ref.fasta',
                     dest = 'ref_fn', default = None,
                     help = 'reference genome, indexed with \'samtools faidx\', '
                            'needed for bias correction' )

    ap.add_argument( '--debug', action = 'store_true', default =  False,
                     help = 'run without launching multiple processes (useful '
                            'for debugging.)'  )

    args = ap.parse_args()


    if args.verbose:
        msg.verbosity( peakolator.LOG_BLAB )
        msg.write( 'verbose output enabled\n', peakolator.LOG_BLAB )


    # check files
    for fn in [ args.reads_fn, args.ref_fn, args.bias_fn ]:
        if fn is not None and not isfile(fn):
            msg.write( 'Error: can\'t open file %s.\n' % fn, peakolator.LOG_ERROR )

    if not isfile( args.reads_fn + '.bai' ):
        msg.write( 'Error: {fn} is not indexed. Run: \'samtools '
                   ' index {fn}\'\n'.format( args.reads_fn ) )

    if args.ref_fn is not None and not isfile( args.ref_fn + '.fai' ):
        msg.write( 'Error: {fn} is not indexed. Run: \'samtools '
                   ' faidx {fn\'\n'.format( args.ref_fn ) )



    # chromosome lengths
    chrom_sizes = peakolator.get_chrom_sizes_from_bam( args.reads_fn )


    # sequencing bias
    bias = None
    if args.ref_fn is not None and args.bias_fn is not None:
        msg.write( 'bias correction enabled\n' )
        bias = peakolator.sequencing_bias()
        bias.load( args.ref_fn, args.reads_fn, args.bias_fn )


    # dataset
    data = peakolator.dataset()
    data.init( args.reads_fn, bias )


    blacklist = get_transcript_extents( reduce( operator.add,
                                                map( parse_gtf_bed, args.ignore),
                                                [] ) )
    predictions = []

    # Iterative Scan
    # --------------
    # 1. Exclude regions known to be transcribed.
    # 2. Train the background model on all other regions.
    # 3. Scan all other regions.
    # 4. If new transcribed regions were found, go to (1),
    #    otherwise halt.
    #
    # Two magic numbers prevent this from running unreasonably long:
    #   At most this many iterations are preformed:
    iteration_limit   = 20
    #   Halt if fewer than this many trascribed regions are found:
    iteration_min_new = 50


    for i in range(iteration_limit):

        whitelist = get_interval_gaps( blacklist, chrom_sizes, stranded = args.stranded )

        n = sum( len(rows) for rows in whitelist.itervalues() )
        m = sum( end - start + 1 for (start,end,_) in chain( *whitelist.itervalues() ) )

        msg.write( '\nIteration %d: %d regions, %d nt\n' % (i+1, n, m) )
        msg.write( '=======================================\n\n' )

        ps = scan( args, data, chrom_sizes, whitelist )

        msg.write( 'Iteration %d complete: %d regions found.\n' % (i+1, len(ps)))

        predictions.extend( ps )
        for (seqname,start,end,strand,pval,name) in ps:
            blacklist[seqname].append( (start,end,strand) )

        if len(ps) < iteration_min_new: break


    for (i,prediction) in enumerate(predictions):
        (seqname,start,end,strand,pval,name) = prediction

        name = '%s-%d' % (name, i+1)

        stdout.write( '{chrom}\t{start}\t{end}\t{name}\t{score}'.format(
            chrom = seqname,
            start = start,
            end   = end+1,   # adjust for end-exclusiveness of bed
            name  = name,
            score = pval ) )
        if strand >= 0:
            stdout.write( '\t%s' % ('+' if strand == 0 else '-') )
        stdout.write( '\n' )


if __name__ == '__main__':
    main()
    msg.write( 'finished main', peakolator.LOG_BLAB )



